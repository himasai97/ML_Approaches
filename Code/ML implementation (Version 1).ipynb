{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn import neighbors\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import r2_score, mean_squared_error,mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the csv files from the GitHub account\n",
    "\n",
    "url_flux = \"https://raw.githubusercontent.com/himasai97/ML_Approaches/master/Data/flux.csv\"\n",
    "url_phen = \"https://raw.githubusercontent.com/himasai97/ML_Approaches/master/Data/Phenotype_Measurements.csv\"\n",
    "\n",
    "\n",
    "download_flux = requests.get(url_flux).content\n",
    "download_phen = requests.get(url_phen).content\n",
    "\n",
    "\n",
    "# Reading the downloaded content and turning it into a pandas dataframes\n",
    "\n",
    "flux = pd.read_csv(io.StringIO(download_flux.decode('utf-8')))\n",
    "y = pd.read_csv(io.StringIO(download_phen.decode('utf-8')))\n",
    "\n",
    "\n",
    "#Extracting data relevant to input flux\n",
    "\n",
    "l =list(range(1,16))+list(range(17,20))+list(range(22,26))+list(range(27,38))+[\"12R\",\"14R\"];\n",
    "FluxNames = [\"V\"+str(num1) for num1 in l];\n",
    "MetNames = [\"Y\"+str(num2) for num2 in range(2,25)];\n",
    "\n",
    "df = flux[FluxNames];\n",
    "X = df.round(9);\n",
    "\n",
    "#Getting a list of Phenotype names\n",
    "Phen_arr = y.columns;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Dataset\n",
    "\n",
    "## Handling NaN Values\n",
    "Missing values in the phenotype measurements are represented as NaNs. For preprocessing, we remove the rows in the flux data that corresponding to the missing values in phenotype measurements. This function is implemented using a user defined routine: **rem_nan**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Removing NaN\n",
    "def rem_nan(X,y,Phen):\n",
    "    y_i = y[Phen]\n",
    "    #extacting index positions where y_i has NaN values\n",
    "    idx = y_i.index[y_i.apply(np.isnan)] #to get the row names\n",
    "    Nan_idx=[(np.where(y.index==idx[i])[0][0]) for i in range(len(idx))] #to get the row numbers for X_i\n",
    "    X_i = X.drop(Nan_idx,axis =0)\n",
    "    y_i = y_i.drop(idx,axis =0)\n",
    "    return X_i, y_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the Data\n",
    "Most of the machine learning algorithms are designed with the assumption that all independent features present in the data vary on comparable scales. So, when the input features have very different scales, it can degrade the predictive performance of these algorithms. Unscaled data can also lead to longer times or even prevent convergence for many gradient-based estimators. Thus, scaling the data is an important step in data preprocessing for many ML approaches. Among the many scikit-learn methods available for this purpose, we use **MinMaxScaler** as it preserves the shape of the original distribution.\n",
    "\n",
    "For each value in an input feature, MinMaxScaler subtracts the minimum value in the feature and then divides by the range. The range is the difference between the maximum and minimum values in the feature. The values returned by MinMaxScaler are thus between 0 and 1.\n",
    "\n",
    "### Implementation\n",
    "In a typical machine learning workflow, we need to apply the above transformations at least twice. Once when training the model and again on any new data we want to predict on. To automate this workflow, we use Scikit-learn **Pipeline** method, as it sequentially applies the transformer and the estimator on a given dataset. The Pipeline approach is especially useful when applying cross-validation where each set of training data needs to be transformed before being fed into the ML estimator. \n",
    "\n",
    "\n",
    "# Hyperparamter Optimization\n",
    "For a machine learning algorithm, hyperparameters are set before training and supplied to the model. They are different from the model parameters that are learned during training by the machine. Hyperparameters are tuned by the user to achieve optimal model performance. We use **GridSearchCV**, a library function from sklearnâ€™s *model_selection package*. From a given grid of hyperparameter values, defined using the *param-grid* parameter, it exhaustively loops through all combinations to optimize the given model (*estimator*) using cross-validation. The performance of the selected hyper-parameters and the trained model is then measured on a dedicated evaluation set that was not used during the model selection step. The number of folds used in the cross-validation (*cv*) and the metric used for evaluation (*scoring*) are both defined along with the *estimator* and the *param-grid* parameters in the GridSearchCV function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up parameters for the ML approach\n",
    "\n",
    "# Defining a 10-repeat 5-fold Cross-validation\n",
    "k_fold = RepeatedKFold(n_splits=5, n_repeats=10)\n",
    "\n",
    "#Defining model learner for the three ML approaches - learner\n",
    "l_SVR = SVR(kernel='rbf')\n",
    "l_XGB = XGBRegressor() \n",
    "l_KNN = neighbors.KNeighborsRegressor()\n",
    "\n",
    "learner = [l_XGB, l_KNN, l_SVR]\n",
    "\n",
    "# Defining a grid of hyperparameter values for each ML approach - tuned_param\n",
    "t_SVR = [{'learner__epsilon':[0.05,0.01,0.1,0.001],'learner__gamma': [5,0.1,0.8,50], 'learner__C': [5,80, 1000, 5000]}]\n",
    "\n",
    "t_XGB = [{\"learner__learning_rate\"    : [0.05, 0.10, 0.20, 0.30 ] ,\n",
    "           \"learner__max_depth\"        : [3,4,5,6,8,12 ],\n",
    "           \"learner__min_child_weight\" : [7,5,3,1],\n",
    "           \"learner__gamma\"  : [ 0.0,0.1,0.3,0.4  ],\n",
    "           \"learner__colsample_bytree\" : [ 0.3,0.4,0.5,0.7 ] }]\n",
    "\n",
    "t_KNN = [{'learner__n_neighbors': list(range(1, 21)),'learner__weights':['uniform','distance'],'learner__metric':['euclidean','manhattan']}]\n",
    "\n",
    "tuned_param = [t_XGB, t_KNN, t_SVR]\n",
    "\n",
    "#Selecting the ML approach to be implemented using its index position in the learner and tuned_param arrays - ml_idx\n",
    "ml_idx = int(input('Type the integer corresponding to the ML model to be implemented: 0 for XGB, 1 for kNN, 2 for SVR'))\n",
    "\n",
    "#For Scaling the Data\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# Defining the pipeline object with a scaler and a leaner model.\n",
    "steps = [('scaler', min_max_scaler), ('learner', learner[ml_idx])]\n",
    "pipeline = Pipeline(steps) \n",
    "\n",
    "# Implementation of GridSearch to tune the pipeline method for best score \n",
    "# from the given grid of hyper-parameters using cross-validation.\n",
    "clf = GridSearchCV(estimator=pipeline,\n",
    "       param_grid= tuned_param[ml_idx], n_jobs=-1, cv=k_fold,scoring ='r2',return_train_score=True)\n",
    "\n",
    "# Here setting n-jobs as -1 ensures that all available processors are used in the GridSearch implementation \n",
    "# and as the name implies, return_train_score is set to True to return the training scores evaluated for the different folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for Phen_idx in range(1,26):\n",
    "\n",
    "    Phen = Phen_arr[Phen_idx]\n",
    "\n",
    "    X_i, y_i = rem_nan(X,y,Phen)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_i, y_i, test_size=0.2, random_state=7)\n",
    "\n",
    "\n",
    "    clf.fit(X_train,y_train)\n",
    "    Param = clf.best_estimator_\n",
    "    train_r2 = clf.best_score_\n",
    "    \n",
    "    #For final evaluation of the trained model, test data is used\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    R2 = r2_score(y_test,y_pred)\n",
    "    MSE = mean_squared_error(y_test, y_pred)\n",
    "    MAE = mean_absolute_error(y_test, y_pred)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_list_as_row(file_name, list_of_elem):\n",
    "    # Open file in append mode\n",
    "    with open(file_name, 'a+', newline='') as write_obj:\n",
    "        # Create a writer object from csv module\n",
    "        csv_writer = writer(write_obj)\n",
    "        # Add contents of list as last row in the csv file\n",
    "        csv_writer.writerow(list_of_elem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
