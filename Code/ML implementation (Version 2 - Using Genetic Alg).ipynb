{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn import neighbors\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import r2_score, mean_squared_error,mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "\n",
    "#To import the genetic algorithm method\n",
    "from genalg import GA_fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X has 225 sample points corresponding to the 35 fluxes.\n",
      "y has 225 sample points corresponding to the 25 phenotypes.\n"
     ]
    }
   ],
   "source": [
    "# Downloading the csv files from the GitHub account\n",
    "\n",
    "url_metabolic = \"https://raw.githubusercontent.com/himasai97/ML_Approaches/master/Data/flux.csv\"\n",
    "url_phen = \"https://raw.githubusercontent.com/himasai97/ML_Approaches/master/Data/Phenotype_Measurements.csv\"\n",
    "\n",
    "\n",
    "download_metabolic = requests.get(url_metabolic).content\n",
    "download_phen = requests.get(url_phen).content\n",
    "\n",
    "\n",
    "# Reading the downloaded content and turning it into a pandas dataframes\n",
    "\n",
    "metabolic = pd.read_csv(io.StringIO(download_metabolic.decode('utf-8'))) #Input metabolic data of fluxes and metabolites\n",
    "y = pd.read_csv(io.StringIO(download_phen.decode('utf-8'))) #Output Phenotype values\n",
    "\n",
    "#Extracting data relevant to phenotypes under study\n",
    "#because the study is to understand the effect of transgenic modifications on the traits of the plant,\n",
    "#height, volume and diameter are more relevant to the study than their growth rates. Hence the three rates are being dropped.\n",
    "y = y.drop(columns=['Row_1', 'HeightRate','DiameterRate', 'VolumeRate']) #removing the label, and the three irrelevant phenotypes\n",
    "\n",
    "#Extracting data relevant to input flux\n",
    "\n",
    "l =list(range(1,16))+list(range(17,20))+list(range(22,26))+list(range(27,38))+[\"12R\",\"14R\"];\n",
    "FluxNames = [\"V\"+str(num1) for num1 in l]; #35 fluxes\n",
    "MetNames = [\"Y\"+str(num2) for num2 in range(2,25)];\n",
    "\n",
    "flux = metabolic[FluxNames];\n",
    "X = flux.round(9);\n",
    "\n",
    "#Getting a list of the Phenotype names\n",
    "Phen_arr = y.columns;\n",
    "\n",
    "#Check the size of the data: X-(#samples,#fluxes) and y-(#samples,#phenotypes), where #fluxes=35, #phenotypes=25\n",
    "print (\"X has {} sample points corresponding to the {} fluxes.\".format(*X.shape))\n",
    "print (\"y has {} sample points corresponding to the {} phenotypes.\".format(*y.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Dataset\n",
    "\n",
    "## Handling NaN Values\n",
    "Missing values in the phenotype measurements are represented as NaNs. For preprocessing, we remove the rows in the flux data that corresponding to the missing values in phenotype measurements. This function is implemented using a user defined routine: **rem_nan**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Removing NaN\n",
    "def rem_nan(X,y,Phen):\n",
    "    y_i = y[Phen]\n",
    "    #extacting index positions where y_i has NaN values\n",
    "    idx = y_i.index[y_i.apply(np.isnan)] #to get the row names\n",
    "    Nan_idx=[(np.where(y.index==idx[i])[0][0]) for i in range(len(idx))] #to get the row numbers for X_i\n",
    "    X_i = X.drop(Nan_idx,axis =0)\n",
    "    y_i = y_i.drop(idx,axis =0)\n",
    "    return X_i, y_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the Data\n",
    "Most of the machine learning algorithms are designed with the assumption that all independent features present in the data vary on comparable scales. So, when the input features have very different scales, it can degrade the predictive performance of these algorithms. Unscaled data can also lead to longer times or even prevent convergence for many gradient-based estimators. Thus, scaling the data is an important step in data preprocessing for many ML approaches. Among the many scikit-learn methods available for this purpose, we use **MinMaxScaler** as it preserves the shape of the original distribution.\n",
    "\n",
    "For each value in an input feature, MinMaxScaler subtracts the minimum value in the feature and then divides by the range. The range is the difference between the maximum and minimum values in the feature. The values returned by MinMaxScaler are thus between 0 and 1.\n",
    "\n",
    "### Implementation\n",
    "In a typical machine learning workflow, we need to apply the above transformations at least twice. Once when training the model and again on any new data we want to predict on. To automate this workflow, we use Scikit-learn **Pipeline** method, as it sequentially applies the transformer and the estimator on a given dataset. The Pipeline approach is especially useful when applying cross-validation where each set of training data needs to be transformed before being fed into the ML estimator. \n",
    "\n",
    "\n",
    "# Hyperparamter Optimization\n",
    "For a machine learning algorithm, hyperparameters are set before training and supplied to the model. They are different from the model parameters that are learned during training by the machine. Hyperparameters are tuned by the user to achieve optimal model performance. We use **GridSearchCV**, a library function from sklearnâ€™s *model_selection package*. From a given grid of hyperparameter values, defined using the *param-grid* parameter, it exhaustively loops through all combinations to optimize the given model (*estimator*) using cross-validation. The performance of the selected hyper-parameters and the trained model is then measured on a dedicated evaluation set that was not used during the model selection step. The number of folds used in the cross-validation (*cv*) and the metric used for evaluation (*scoring*) are both defined along with the *estimator* and the *param-grid* parameters in the GridSearchCV function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up parameters for the ML approach\n",
    "\n",
    "# Defining a 10-repeat 5-fold Cross-validation\n",
    "k_fold = RepeatedKFold(n_splits=5, n_repeats=10)\n",
    "\n",
    "#Defining model learner for the three ML approaches - learner\n",
    "l_SVR = SVR(kernel='rbf')\n",
    "l_XGB = XGBRegressor() \n",
    "l_KNN = neighbors.KNeighborsRegressor()\n",
    "\n",
    "learner = [l_XGB, l_KNN, l_SVR]\n",
    "\n",
    "# Defining a grid of hyperparameter values for each ML approach - tuned_param\n",
    "t_SVR = [{'learner__epsilon':[0.05,0.01,0.1,0.001],'learner__gamma': [5,0.1,0.8,50], 'learner__C': [5,80, 1000, 5000]}]\n",
    "\n",
    "t_XGB = [{\"learner__learning_rate\"    : [0.05, 0.10, 0.20, 0.30 ] ,\n",
    "           \"learner__max_depth\"        : [3,4,5,6,8,12 ],\n",
    "           \"learner__min_child_weight\" : [7,5,3,1],\n",
    "           \"learner__gamma\"  : [ 0.0,0.1,0.3,0.4  ],\n",
    "           \"learner__colsample_bytree\" : [ 0.3,0.4,0.5,0.7 ] }]\n",
    "\n",
    "t_KNN = [{'learner__n_neighbors': list(range(1, 21)),'learner__weights':['uniform','distance'],'learner__metric':['euclidean','manhattan']}]\n",
    "\n",
    "tuned_param = [t_XGB, t_KNN, t_SVR]\n",
    "\n",
    "#Selecting the ML approach to be implemented using its index position in the learner and tuned_param arrays - ml_idx\n",
    "ml_idx = int(input('Type the integer corresponding to the ML model to be implemented: 0 for XGB, 1 for kNN, 2 for SVR'))\n",
    "\n",
    "#For Scaling the Data\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# Defining the pipeline object with a scaler and a leaner model.\n",
    "steps = [('scaler', min_max_scaler), ('learner', learner[ml_idx])]\n",
    "pipeline = Pipeline(steps) \n",
    "\n",
    "# Implementation of GridSearch to tune the pipeline method for best score \n",
    "# from the given grid of hyper-parameters using cross-validation.\n",
    "clf = GridSearchCV(estimator=pipeline,\n",
    "       param_grid= tuned_param[ml_idx], n_jobs=-1, cv=k_fold,scoring ='r2',return_train_score=True)\n",
    "\n",
    "# Here setting n-jobs as -1 ensures that all available processors are used in the GridSearch implementation \n",
    "# and as the name implies, return_train_score is set to True to return the training scores evaluated for the different folds.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Algorithm\n",
    "In version-1 of the ML implementation, we described the ML approaches designed to predict the 25 lignin and wood properties using the 35 monolignol steady-state fluxes as input. Here, we proposed the implementation of genetic algorithm, a feature selection technique, to further improve the performance of these models. In this implementation each chromosome in the population is represented by 35 binary genes for the 35 input fluxes. Here, the binary value of each gene represents whether the corresponding feature is to be included in the model or not.\n",
    "\n",
    "### Implementation\n",
    "Genetic Algorithm for Feature Selection: **GA_fs** with inputs:\n",
    "\n",
    "- *X*: input with all fluxes\n",
    "- *y*: true values of the predicted phenotype\n",
    "- *clf*: the ML model to be tuned and used for prediction\n",
    "- *Phen*: Name of the phenotype to be predicted\n",
    "\n",
    "and outputs:\n",
    "\n",
    "- *F_array*: array of selected features \n",
    "- *best_clf*: ML model optimized for the selected set of input features\n",
    "- *train_r2*: R<sup>2</sup> evaluated for the cross-validation folds in the grid search algorithm\n",
    "\n",
    "Using this method, we obtained a set of input features and hyperparameter values for each ML model to obtain better model efficiency when compared with the implementation in Version 1. To measure efficiency, we evaluated the final R<sup>2</sup> score using the untouched test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise arrays for the different metrics\n",
    "R2 = []\n",
    "train_r2_arr = []\n",
    "\n",
    "\n",
    "#Implementation\n",
    "for Phen_idx in range(25):\n",
    "\n",
    "    Phen = Phen_arr[Phen_idx]\n",
    "\n",
    "    X_i, y_i = rem_nan(X,y,Phen)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_i, y_i, test_size=0.2, random_state=7)\n",
    "\n",
    "\n",
    "    (F_array,best_clf,train_r2) = GA_fs(X_train,y_train,clf,Phen)\n",
    "    \n",
    "    #Saving the model\n",
    "    model_file = 'model_'+Phen+'.sav'\n",
    "    pickle.dump(best_clf, open(model_file, 'wb'))\n",
    "    \n",
    "    #Saving Selected features\n",
    "    F_file = 'features_'+Phen+'.data'\n",
    "    with open(F_file, 'wb') as filehandle:\n",
    "        # store the data as binary data stream\n",
    "        pickle.dump(F_array, filehandle)\n",
    "    \n",
    "    #To reload the saved model\n",
    "    loaded_model = pickle.load(open(model_file, 'rb'))\n",
    "    \n",
    "    #To reload the saved features array\n",
    "    with open(F_file, 'rb') as filehandle:\n",
    "        # read the data as binary data stream\n",
    "        selected_f = pickle.load(filehandle)\n",
    "\n",
    "    #For final evaluation of the trained model, test data is used\n",
    "    X_test_f = X_test[selected_f]\n",
    "    y_pred = loaded_model.predict(X_test_f)\n",
    "\n",
    "    R2[Phen_idx] = r2_score(y_test,y_pred) \n",
    "    train_r2_arr[Phen_idx] = train_r2\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Results\n",
    "\n",
    "The $R^2$ scores are compared with the results from version 1 for each ML approach and visualized below.\n",
    "\n",
    "## Figure 1: kNN model\n",
    "<img src=\"Images/knn_compare.PNG\">\n",
    "          \n",
    "## Figure 2: SVR model\n",
    "<img src=\"Images/svr_compare.PNG\">\n",
    "\n",
    "## Figure 3: XGB model\n",
    "<img src=\"Images/xgb_compare.PNG\">\n",
    "\n",
    "\n",
    "## Analysis\n",
    "\n",
    "For kNN model, there was an increase in R<sup>2</sup> value by at least 0.1 for 12 lignin and wood properties, including lignin content, SG ratio, S subunits, G subunits, $\\beta-O-4$ linkage, $\\beta-5$ linkage, endgroups, saccharification efficiency of glucose unpretreated samples, saccharification efficiency of xylose unpretreated samples, saccharification efficiency of xylose pretreated samples, height, and diameter (Fig 1). However, for 5 phenotype traits, including H subunits, phydroxybenzoate, CL ratio, aldehydes, and volume, there was a neglible increase in $R^2$ value, by less than 0.02. \n",
    "\n",
    "Using Genetic Algorithm, SVR model showed an increase in $R^2$ score by at least 0.1 for 11 lignin and wood properties, including SG ratio, S subunits, Aldhehydes, $\\beta-5$ linkage, endgroups, relative density, modulus of elasticity, saccharification efficiency of glucose pretreated samples, saccharification efficiency of xylose pretreated samples, height, and volume (Fig 2). \n",
    "\n",
    "For 4 lignin and wood properties, including H subunits, phydroxybenzoate, xylose, and total sugar, there was very little increase in $R^2$ score, by less than 0.02. Similarly for XGB model, we found that the $R^2$ value increased by at least 0.1 for 13 lignin and wood properties, including SG ratio, S subunits, G subunits, CL ratio, aldehydes, $\\beta-O-4$ linkage, $\\beta-5$ linkage, $\\beta-1$ linkage, height, diameter, volume, relative density, and saccharification efficiency of xylose pretreated samples (Fig 3). However for H subunits and $\\beta-\\beta$ linkage, there was a negligible increase in $R^2$ values for XGB model, by less than 0.02. \n",
    "\n",
    "We can infer that the lack of improvement in $R^2$ values of ML models for certain phenotype traits, even with the reduction in the number of input features, is an indicator of a poor fit by the models in the prediction of these phenotype traits.\n",
    "\n",
    "# Comparison with the Original Model\n",
    "\n",
    "Based on the results illustrated in Fig 4, the original random forest method performed better than the ML approaches for only 1 phenotype trait, H subunits. We see that XGB performed better for 8 lignin and wood properties, including SG ratio, phydroxybenzoate, aldehydes, glucose, total sugar, saccharification efficiency of glucose unpretreated samples, saccharification efficiency of glucose pretreated samples, and saccharification efficiency of xylose pretreated samples. For 7 of these lignin and wood properties, except total sugar, SVR also performed better than the original random forest method. kNN, on the other hand, performed better than the original model for only 4 properties, including SG ratio, glucose, saccharification efficiency of xylose unpretreated samples, and saccharification efficiency of glucose pretreated samples. However, all three ML models performed as good as the original random forest model for 7 of the lignin and wood properties (increase in $R^2$ by less than 0.01), including S subunits, G subunits, CL ratio, $\\beta-1$ linkage, $\\beta-\\beta$ linkage, relative density, and modulus of elasticity. \n",
    "\n",
    "## Figure 4: With Original Random Forest Method as Baseline Model\n",
    "<img src=\"Images/comp_gen.PNG\">\n",
    "\n",
    "Having the original random forest model developed by Matthews as the baseline model, we can conclude that the three machine learning approaches implemented with genetic algorithm produce good predictions for 24 of the 25 phenotype traits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
